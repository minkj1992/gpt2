{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "## refs\n",
    "\n",
    "### paper\n",
    "- https://arxiv.org/pdf/1706.03762\n",
    "\n",
    "### vis\n",
    "- https://bbycroft.net/llm\n",
    "- https://docs.google.com/spreadsheets/d/10O-amPDV4zvnZZedlqX33pLFxlO4jj8CbhY6jzdk5rw/edit?gid=260373902#gid=260373902\n",
    "- https://jalammar.github.io/illustrated-transformer/\n",
    "- https://jalammar.github.io/illustrated-gpt2/\n",
    "\n",
    "### impl\n",
    "- karpathy: https://github.com/karpathy/build-nanogpt\n",
    "- https://nlp.seas.harvard.edu/annotated-transformer/#attention-visualization\n",
    "- https://course.fast.ai/Lessons/lesson24.html\n",
    "- https://github.com/jadore801120/attention-is-all-you-need-pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    .output-plaintext, .output-stream, .output {\n",
       "        font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\" !important;\n",
       "        line-height: 1.2 !important;\n",
       "        font-size: 15px !important;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(r\"\"\"\n",
    "<style>\n",
    "    .output-plaintext, .output-stream, .output {\n",
    "        font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\" !important;\n",
    "        line-height: 1.2 !important;\n",
    "        font-size: 15px !important;\n",
    "    }\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 환경 세팅\n",
    "\n",
    "\n",
    "- 라이브러리 다운로드\n",
    "\n",
    "```bash\n",
    "conda create -n gpt python=3.12 -y\n",
    "conda activate gpt\n",
    "# https://github.com/explosion/spaCy/issues/13528\n",
    "conda install \"numpy>=1.19.0,<2.0.0\" spacy -y  \n",
    "conda install pytorch::pytorch torchvision torchaudio torchtext -c pytorch -y\n",
    "conda install matplotlib tensorboard seaborn -y\n",
    "```\n",
    "\n",
    "- gpu setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to define the models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "#to load,iterate and process the dataset\n",
    "import torchtext\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data.metrics import bleu_score\n",
    "from torchtext.data import Field,BucketIterator\n",
    "\n",
    "#to visualize loss plots on localhost while training\n",
    "# See https://pytorch.org/docs/stable/tensorboard.html for more details\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.3.1', '0.18.1', '0.6.0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__, torchvision.__version__, torchtext.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#miscallaneous imports\n",
    "\n",
    "import math\n",
    "import spacy\n",
    "import random\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Using MPS (Metal Performance Shaders) on Mac\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "def setup_device():\n",
    "    device = get_device()\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "        torch.cuda.empty_cache()\n",
    "    elif device.type == \"mps\":\n",
    "        print(\"Using MPS (Metal Performance Shaders) on Mac\")\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "# 사용 예시\n",
    "device = setup_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "SEED = 1024\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 데이터\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_tokenizers():\n",
    "    # Download tokenizer from spacy\n",
    "    try:\n",
    "        spacy_de = spacy.load(\"de_core_news_sm\") # german\n",
    "    except IOError:\n",
    "        os.system(\"python -m spacy download de_core_news_sm\")\n",
    "        spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "    try:\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\") # english\n",
    "    except IOError:\n",
    "        os.system(\"python -m spacy download en_core_web_sm\")\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    return spacy_de, spacy_en\n",
    "\n",
    "\n",
    "spacy_de, spacy_en = load_tokenizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'wish', 'you', 'all', 'the', 'best']\n",
      "['Ich', 'wünsche', 'Ihnen', 'alles', 'Gute']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_english(eng_text):\n",
    "    return [token.text for token in spacy_en.tokenizer(eng_text)]\n",
    "\n",
    "def tokenize_german(german_text):\n",
    "    return [token.text for token in spacy_de.tokenizer(german_text)]\n",
    "\n",
    "print(tokenize_english('I wish you all the best'))\n",
    "print(tokenize_german('Ich wünsche Ihnen alles Gute'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `torchtext.data.Field`\n",
    "\n",
    "- `lower`: 텍스트를 소문자로 변환할지 여부를 지정합니다.\n",
    "- `batch_first`: 배치 차원을 첫 번째로 하는 텐서를 생성할지 여부를 지정합니다.\n",
    "\n",
    "\n",
    "또한 transformer의 encoder에 들어갈 input인 `source`, output인 `target`을 독일어 -> 영어로 지정합니다.\n",
    "\n",
    "- `source`: german\n",
    "- `target`: english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n",
    "\n",
    "UNK = '<unk>' # 0\n",
    "PAD = '<pad>' # 1\n",
    "SOS = '<sos>' # 2\n",
    "EOS = '<eos>' # 3\n",
    "\n",
    "source_process_pipeline = Field(tokenize = tokenize_german,\n",
    "                       init_token = SOS,\n",
    "                       eos_token = EOS,\n",
    "                       pad_token= PAD,\n",
    "                       unk_token=UNK,\n",
    "                       lower = True,\n",
    "                       batch_first = True)\n",
    "target_process_pipeline = Field(tokenize = tokenize_english,\n",
    "                       init_token = SOS,\n",
    "                       eos_token = EOS,\n",
    "                       pad_token= PAD,\n",
    "                       unk_token=UNK,\n",
    "                       lower = True,\n",
    "                       batch_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Multi30k](https://pytorch.org/text/stable/datasets.html#multi30k)\n",
    "\n",
    "이제 Multi30k 독일어-영어 번역 작업을 사용한 실제 예제를 살펴보겠습니다. 이 작업은 논문에서 다룬 WMT 작업(대규모 기계 번역 task)보다 훨씬 규모가 작지만, 여전히 전체 시스템을 잘 보여줄 수 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/text/issues/1756\n",
    "from torchtext.datasets import Multi30k\n",
    "\n",
    "Multi30k.urls = [\n",
    "    \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\",\n",
    "    \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\",\n",
    "    \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/mmt_task1_test2016.tar.gz\",\n",
    "]\n",
    "\n",
    "train_data, validation_data, test_data = Multi30k.splits(exts = ('.de','.en'),\n",
    "                                                       fields = (source_process_pipeline,\n",
    "                                                                target_process_pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7853\n",
      "5893\n"
     ]
    }
   ],
   "source": [
    "source_process_pipeline.build_vocab(train_data,min_freq=2)\n",
    "target_process_pipeline.build_vocab(train_data,min_freq=2)\n",
    "\n",
    "print(len(source_process_pipeline.vocab))\n",
    "print(len(target_process_pipeline.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<unk>', 0), ('<pad>', 1), ('<sos>', 2), ('<eos>', 3), ('.', 4), ('ein', 5), ('einem', 6), ('in', 7), ('eine', 8), (',', 9), ('und', 10), ('mit', 11), ('auf', 12), ('mann', 13), ('einer', 14), ('der', 15), ('frau', 16), ('die', 17), ('zwei', 18), ('einen', 19)]\n",
      "[('<unk>', 0), ('<pad>', 1), ('<sos>', 2), ('<eos>', 3), ('a', 4), ('.', 5), ('in', 6), ('the', 7), ('on', 8), ('man', 9), ('is', 10), ('and', 11), ('of', 12), ('with', 13), ('woman', 14), (',', 15), ('two', 16), ('are', 17), ('to', 18), ('people', 19)]\n"
     ]
    }
   ],
   "source": [
    "print(list(source_process_pipeline.vocab.stoi.items())[:20])\n",
    "print(list(target_process_pipeline.vocab.stoi.items())[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## [Bucket Iterator](https://torchtext.readthedocs.io/en/latest/data.html?highlight=bucket#bucketiterator)\n",
    "\n",
    "Pytorch의 dataloader와 비슷한 역할을 한다. 하지만 dataloader 와 다르게 비슷한 길이의 문장들끼리 \n",
    "batch를 만들기 때문에 padding의 개수를 최소화할 수 있다. 내부적으로 [torchtext.data.pool](https://torchtext.readthedocs.io/en/latest/data.html?highlight=bucket#bucketiterator)을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, validation_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, validation_data, test_data), \n",
    "     batch_size = BATCH_SIZE,\n",
    "     device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8f/The-Transformer-model-architecture.png/580px-The-Transformer-model-architecture.png)\n",
    "\n",
    "\n",
    "\n",
    "## 1-1. Big picture\n",
    "\n",
    "@TODO encoder-decoder그림\n",
    "@TODO linearSoftmax 그림\n",
    "@padding mask 설명, @look-ahead mask 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.functional import log_softmax\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, i_embed, o_embed, generator):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.i_embed = i_embed\n",
    "        self.o_embed = o_embed\n",
    "        self.generator = generator # LinearSoftmax\n",
    "\n",
    "    def forward(self, inp, oup, in_mask, out_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_mask (Tensor): Padding mask applied to the input sequence\n",
    "            out_mask (Tensor): Look-ahead mask applied to the output sequence\n",
    "        \"\"\"\n",
    "        enc_output = self.encoder(self.i_embed(inp), in_mask)\n",
    "        self.decoder(\n",
    "            self.o_embed(oup), out_mask, # masked attention return is Q\n",
    "            enc_output, in_mask # K, V\n",
    "        )\n",
    "\n",
    "# Last layer\n",
    "class LinearSoftmax(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(LinearSoftmax, self).__init__()\n",
    "        # (... ,in) -> (..., out)\n",
    "        self.proj = nn.Linear(in_features=d_model, out_features=vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return log_softmax(self.proj(x), dim=-1) # out dim(vocab)에 대해서 softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log_softmax\n",
    "> https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax\n",
    "\n",
    "### 1. log_softmax 사용하는 이유\n",
    "\n",
    "1. 단조 함수 (monotonic function)\n",
    "Softmax 함수를 통해 입력 벡터를 확률 분포로 변환한 후, 이 확률 분포에서 가장 높은 값을 가지는 요소, 즉 \"max값\"을 알아내고자 합니다. 이때 softmax 함수의 결과에 log를 취하면, max값을 찾는 과정에서 변화가 없습니다. 왜냐하면 log 함수는 monotonic한 함수이기 때문에 입력 값의 순서를 유지하며, softmax의 결과에서 가장 큰 값이었던 요소는 log를 취해도 여전히 가장 큰 값으로 남아 있습니다.\n",
    "\n",
    "2. overflow 방지\n",
    "또한, log를 취함으로써 얻는 이점 중 하나는 수치적 안정성입니다. softmax 함수는 지수 함수를 포함하므로, 입력 값이 크면 결과값도 매우 크게 증가할 수 있습니다. \n",
    "이때 $e^\\infty$ 인 경우 numpy float으로 표현할 수 있는 값을 넘어갈 수 있어 overflow가 발생할 수 있습니다. 이를 방지하기 위해서 pytorch에서는 `log_softmax()`, `logsumexp()`를 제공합니다.\n",
    "\n",
    "따라서 log(softmax) 함수는 softmax의 확률 값을 변환하되, 확률 분포에서의 max값을 유지하며, 수치적으로 안정성이 더 좋은 결과를 제공하는 함수로 사용될 수 있습니다.\n",
    "\n",
    "다음으로 어떻게 log_softmax가 exp를 안정적으로 계산하여, overflow를 방지하고 있는지를 살펴보도록 하겠습니다.\n",
    "\n",
    "로그를 취하면 큰 값이 덧셈으로 변환되므로 수치적으로 안정한 계산이 가능합니다.\n",
    "\n",
    "\n",
    "## 2. log\\_softmax 수식\n",
    "\n",
    "log\\_softmax 함수는 주어진 벡터 $\\mathbf{x} = [x_1, x_2, \\ldots, x_n]$에 대해 pytorch에서는 다음과 같이 계산됩니다:\n",
    "\n",
    "$$\n",
    "\\text{log\\_softmax}(x_i) = x_i - c - \\log \\left( \\sum_{j=1}^{n} e^{x_j - c} \\right)\n",
    "$$\n",
    "\n",
    "이때 $c$는 벡터 $ \\mathbf{x} $의 최대값이라고 정의합니다.\n",
    "\n",
    "$ c = \\max(x_1, x_2, \\ldots, x_N) $\n",
    "\n",
    "c를 빼줌으로써, $e^\\text{매우큰수}$ 인 경우를 배제하여 overflow를 방지할 수 있습니다. \n",
    "\n",
    "그럼 이제 위의 수식이 어떻게 나오게 되었는지 아래에서 확인 해보겠습니다.\n",
    "\n",
    "\n",
    "### 원래의 소프트맥스 수식\n",
    "\n",
    "소프트맥스 함수는 다음과 같이 정의됩니다:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\n",
    "$$\n",
    "\n",
    "### 로그 소프트맥스\n",
    "\n",
    "소프트맥스 함수의 출력에 로그를 취하면 다음과 같습니다:\n",
    "\n",
    "$$\n",
    "\\log \\left( \\text{softmax}(x_i) \\right) = \\log \\left( \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}} \\right)\n",
    "$$\n",
    "\n",
    "로그의 성질을 이용하여 분자와 분모를 분리하면:\n",
    "\n",
    "$$\n",
    "\\log \\left( \\text{softmax}(x_i) \\right) = \\log(e^{x_i}) - \\log \\left( \\sum_{j=1}^{n} e^{x_j} \\right)\n",
    "$$\n",
    "\n",
    "이는 다음과 같이 단순화됩니다:\n",
    "\n",
    "$$\n",
    "\\log \\left( \\text{softmax}(x_i) \\right) = x_i - \\log \\left( \\sum_{j=1}^{n} e^{x_j} \\right)\n",
    "$$\n",
    "\n",
    "하지만 여전히 뒤에, exp(x)에서 x가 무한히 클 경우 overflow가 발생할 수 있습니다. 이렇게 뒤쪽에 있는 log sum exp를 pytorch에서는 [torch.logsumexp](https://pytorch.org/docs/stable/generated/torch.logsumexp.html)로 구현해주고 있으며, 아래와 같이 trick을 사용해서 계산합니다.\n",
    "\n",
    "### logsumexp\n",
    "\n",
    "`logsumexp`를 구현해주기 위해서, 먼저, $ c $를 벡터 $ \\mathbf{x} $의 최대값이라고 정의합니다:\n",
    "\n",
    "$ c = \\max(x_1, x_2, \\ldots, x_N) $\n",
    "\n",
    "이후 아래의 지수함수 성질을 이용합니다.\n",
    "\n",
    "$ e^{a + b} = e^{a} \\cdot e^{b} $\n",
    "\n",
    "지수함수 성질을 통해, 원래 수식을 다음과 같이 재정렬할 수 있습니다:\n",
    "$ e^{x_n} = e^{(x_n - c) + c} = e^{x_n - c} \\cdot e^{c} $\n",
    "\n",
    "#### 3. 지수 함수 성질 적용\n",
    "위의 성질을 원래 수식에 적용해봅시다:\n",
    "$ \\sum_{n=1}^{N} e^{x_n} = \\sum_{n=1}^{N} e^{x_n - c} \\cdot e^{c} $\n",
    "\n",
    "여기서 $e^{c}$는 상수이므로 sigma 밖으로 빼낼 수 있습니다:\n",
    "$ \\sum_{n=1}^{N} e^{x_n} = e^{c} \\sum_{n=1}^{N} e^{x_n - c} $\n",
    "\n",
    "이를 다시 정리하면:\n",
    "\n",
    "$$ \n",
    "sumexp = \\sum_{n=1}^{N} e^{x_n} = e^{c} \\sum_{n=1}^{N} e^{x_n - c}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\log (sumexp) = \\log \\left( e^c \\sum_{j=1}^{n} e^{x_j - c} \\right) = c + \\log \\left( \\sum_{j=1}^{n} e^{x_j - c} \\right)\n",
    "$$\n",
    "\n",
    "이를 이용하여 log\\_softmax 수식을 다시 쓰면:\n",
    "\n",
    "$$\n",
    "\\text{log\\_softmax}(x_i) = x_i - \\log (sumexp)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{log\\_softmax}(x_i) = x_i - c - \\log \\left( \\sum_{j=1}^{n} e^{x_j - c} \\right)\n",
    "$$\n",
    "\n",
    "이 변형을 통해 매우 큰 값에 대해 수치적으로 안정적인 계산이 가능하게 됩니다.\n",
    "\n",
    "\n",
    "## c.f softmax vs log_softmax\n",
    "\n",
    "이제 softmax와 log_softmax를 비교해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax output (overflow): tensor([nan, nan, nan])\n",
      "Log softmax output (stability): tensor([-2000., -1000.,     0.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "inputs = torch.tensor([1000.0, 2000.0, 3000.0])\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = torch.exp(x)\n",
    "    sum_exp_x = torch.sum(exp_x)\n",
    "    return exp_x / sum_exp_x\n",
    "\n",
    "def log_softmax(x):\n",
    "    c = torch.max(x)\n",
    "    logsumexp = torch.log(torch.sum(torch.exp(x - c)))\n",
    "    return x - c - logsumexp\n",
    "\n",
    "\n",
    "print(\"Softmax output (overflow):\", softmax(inputs))\n",
    "\n",
    "# log_softmax_output = F.log_softmax(inputs, dim=0)\n",
    "print(\"Log softmax output (stability):\", log_softmax(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. Encoder layer\n",
    "\n",
    "The encoder is composed of a stack of N = 6 identical layers.\n",
    "\n",
    "\n",
    "\n",
    "### Residual Dropout (5.4)\n",
    ">  We apply dropout [27] to the output of each sub-layer, before it is added to the\n",
    "sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\n",
    "positional encodings in both the encoder and decoder stacks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, padding_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, padding_mask) # norm -> dropout\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "# Layer norm test\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"Construct a layernorm module\n",
    "\n",
    "    d_model 방향으로 평균, 표준편차 계산한다.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features, eps=1e-5):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.w = nn.Parameter(torch.ones(features))\n",
    "        self.b = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True) # x is (batch, sentence, d_model)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.w * (x - mean) / (std + self.eps) + self.b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 논문 상에서 Layer normalization\n",
    "\n",
    "> Unless otherwise noted, the default initialization of layer normalization is to set the adaptive gains to 1 and the biases to 0  ([Layer normalization 6.](https://arxiv.org/pdf/1607.06450))\n",
    "\n",
    "- alpha: 1\n",
    "- beta: 0\n",
    "\n",
    "구현한 layer normalization을 pytorch 구현체와 비교해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 1])\n",
      "tensor([[[-0.8275, -0.0135,  0.6378,  ..., -1.2619,  0.0085, -1.2845],\n",
      "         [ 0.5016, -1.5572,  0.2296,  ...,  0.3337, -0.2966, -0.9650],\n",
      "         [-0.0318,  0.4579, -1.8165,  ..., -0.1260,  0.7774, -0.2929]],\n",
      "\n",
      "        [[ 0.9011, -0.1162, -1.1876,  ..., -2.0658, -1.3579,  2.4320],\n",
      "         [-0.2494,  1.5145,  0.6840,  ...,  0.0284, -0.7935,  0.1723],\n",
      "         [-1.3209,  0.4843,  1.5879,  ...,  0.9951, -1.4229, -1.6999]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[[-0.8283, -0.0135,  0.6384,  ..., -1.2632,  0.0085, -1.2858],\n",
      "         [ 0.5021, -1.5587,  0.2298,  ...,  0.3341, -0.2969, -0.9660],\n",
      "         [-0.0318,  0.4583, -1.8183,  ..., -0.1261,  0.7782, -0.2931]],\n",
      "\n",
      "        [[ 0.9020, -0.1163, -1.1888,  ..., -2.0678, -1.3593,  2.4344],\n",
      "         [-0.2497,  1.5159,  0.6847,  ...,  0.0285, -0.7942,  0.1725],\n",
      "         [-1.3222,  0.4848,  1.5895,  ...,  0.9961, -1.4243, -1.7016]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_length = 3\n",
    "d_model = 512\n",
    "input_tensor = torch.randn(batch_size, seq_length, d_model)\n",
    "print(LayerNorm(d_model)(input_tensor))\n",
    "# check pytorch layer norm\n",
    "print(nn.LayerNorm(d_model)(input_tensor)) # random seed 영향"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer normalization vs Batch normalization\n",
    "\n",
    "![](https://i.sstatic.net/E3104.png)\n",
    "\n",
    "위 그림에서 Feature를 d_model로 생각하면, layer normalization의 경우, 한 문장의 한 토큰이 가진 d_model들의 평균과 분산을 구해서 normalization을 진행합니다. 이는 배치와 문장들에 상관없이 평균 / 분산이 계산됩니다. `(batch, seq_len, 1)`\n",
    "\n",
    "Batch normalization의 경우, 하나의 배치안에 존재하는 모든 sentence 문장들에 대해서 평균을 구합니다. 즉 \"love\"라는 단어와, \"hate\"라는 단어가 각각 512차원으로 하나의 배치의 문장들에 포함되어있을 경우, 0~511 index 각각 값들을 모든 배치안의 모든 문장안의 모든 단어들에 대해서 값들을 얻어와 평균과 분산을 계산합니다. `(1,512)`\n",
    "\n",
    "\n",
    "## 아래 코드 적용\n",
    "주어진 input tensor input_tensor은 (batch_size, seq_length, d_model)의 형태에 대하여\n",
    "\n",
    "`Layer normalization`은 d_model 차원에 대해 각각의 샘플(문장)에 대해 평균과 표준 편차를 계산합니다. 이를 통해 (batch_size, seq_length, 1)의 평균, 표준편차가 만들어집니다. 이 과정을 통해 각 샘플의 각 feature dimension이 평균 0, 분산 1에 가까워지게 됩니다.\n",
    "\n",
    "\n",
    "`Batch normalization`은 한 번에 전체 batch의 모든 샘플에 대해 각 feature dimension을 정규화합니다. 따라서 입력의 모든 샘플에 대해 평균과 표준 편차를 계산하고, 이를 사용하여 정규화를 수행합니다. 이를 통해 평균과 표준편차는 (1, 1, d_model)의 shape를 가집니다. Batch normalization은 전체 batch의 모든 샘플에 대해 계산하기 때문에, 데이터의 분포를 안정화시키고 학습 과정을 안정화시키는 데 도움을 줍니다.\n",
    "\n",
    "\n",
    "**Layer normalization 처럼 각 토큰들에 대해서 독립적으로 평균 / 분산을 계산해서 normalization을 하는 것이 nlp에서는 실험결과 더 효과적이라고 평가합니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [3., 3., 3.,  ..., 3., 3., 3.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [3., 3., 3.,  ..., 3., 3., 3.]]])\n",
      "torch.Size([2, 3, 1])\n",
      "Output Tensor: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_custom_init(batch_size, seq_length, d_model):\n",
    "    input_tensor = torch.zeros(batch_size, seq_length, d_model)\n",
    "    for i in range(seq_length):\n",
    "        init_value = i + 1\n",
    "        input_tensor[:, i, :] = init_value  # 각 seq_length에 따라 d_model의 값을 초기화\n",
    "    return input_tensor\n",
    "\n",
    "input_tensor = test_custom_init(batch_size, seq_length, d_model)\n",
    "print(input_tensor)\n",
    "\n",
    "output_tensor = LayerNorm(d_model)(input_tensor)\n",
    "print(\"Output Tensor:\", output_tensor)\n",
    "\n",
    "# check pytorch layer norm\n",
    "nn.LayerNorm(d_model)(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 3])\n",
      "tensor([[[1., 2., 3.],\n",
      "         [1., 2., 3.],\n",
      "         [1., 2., 3.],\n",
      "         ...,\n",
      "         [1., 2., 3.],\n",
      "         [1., 2., 3.],\n",
      "         [1., 2., 3.]],\n",
      "\n",
      "        [[1., 2., 3.],\n",
      "         [1., 2., 3.],\n",
      "         [1., 2., 3.],\n",
      "         ...,\n",
      "         [1., 2., 3.],\n",
      "         [1., 2., 3.],\n",
      "         [1., 2., 3.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2247, -1.2247, -1.2247,  ..., -1.2247, -1.2247, -1.2247],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.2247,  1.2247,  1.2247,  ...,  1.2247,  1.2247,  1.2247]],\n",
       "\n",
       "        [[-1.2247, -1.2247, -1.2247,  ..., -1.2247, -1.2247, -1.2247],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.2247,  1.2247,  1.2247,  ...,  1.2247,  1.2247,  1.2247]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare with batch norm\n",
    "\n",
    "# pytorch BatchNorm1d input (batch, features seq_length) \n",
    "# (2,3,512) -> (2,512,3)\n",
    "input_tensor_transposed = input_tensor.transpose(1, 2) #  (batch_size, seq_length, d_model) -> (batch_size, d_model, seq_length)\n",
    "print(input_tensor_transposed.shape)\n",
    "print(input_tensor_transposed)\n",
    "nn.BatchNorm1d(d_model)(input_tensor_transposed).transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = deepcopy(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = deepcopy(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
